{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Flow\n",
    "\n",
    "1) Get number of pages\n",
    "2) Parallelise job, making one driver for each page.\n",
    "3) # Do I want to do this in a massive for loop? Not really..But I need to somehow change the page number for each page\n",
    "\n",
    "- Remember: USP == sORFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## *** The workflow *** ## \n",
    "## Built to be flexible and scalable (test with different organisms etc.)\n",
    "## First import modules\n",
    "## Maybe make virtualenv with all the necessary dependencies already installed? - would probs help with later bedtools stuff etc\n",
    "\n",
    "\n",
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.element import Comment\n",
    "import urllib\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import requests\n",
    "import timeit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function definitions\n",
    "\n",
    "## HTML parsing\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method submit in module concurrent.futures.thread:\n",
      "\n",
      "submit(fn, *args, **kwargs) method of concurrent.futures.thread.ThreadPoolExecutor instance\n",
      "    Submits a callable to be executed with the given arguments.\n",
      "    \n",
      "    Schedules the callable to be executed as fn(*args, **kwargs) and returns\n",
      "    a Future instance representing the execution of the callable.\n",
      "    \n",
      "    Returns:\n",
      "        A Future representing the given call.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp = ThreadPoolExecutor(max_workers=5)\n",
    "help(tp.submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oct 4th, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to give this function the URL of each page, and get it to parse each page, find all links\n",
    "## then get all chrom_coords for each page\n",
    "\n",
    "def get_page_deets(page_URL):\n",
    "\n",
    "    ## First get all links on one page\n",
    "    #response = urllib.request.urlopen(URL)\n",
    "#     http = httplib2.Http()\n",
    "#     status, response = http.request(page_URL)\n",
    "\n",
    "    response = s.get(page_URL).text\n",
    "\n",
    "    all_links = []\n",
    "    for link in BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer('a')):\n",
    "        try:\n",
    "            all_links.append(link['href'])\n",
    "        except:\n",
    "            pass\n",
    "    ## Filter out other irrelevant links in the page\n",
    "    SM_prot_codes = [u for u in all_links if 'SmProt.php' in u]\n",
    "\n",
    "\n",
    "\n",
    "    IDs = []\n",
    "    Sequence = []\n",
    "    Length = []\n",
    "    Transcript_ID = []\n",
    "    Gene_ID = []\n",
    "    Location = []\n",
    "    Cell_Lines = []\n",
    "    Data_Source = []\n",
    "    Experiment = []\n",
    "    Experimental_Description = []\n",
    "\n",
    "  \n",
    "    for SM_prot_code in SM_prot_codes:\n",
    "        URL = 'http://bioinfo.ibp.ac.cn/SmProt/' + SM_prot_code\n",
    "        # Think I'll use http.request as faster \n",
    "        #response = urllib.request.urlopen(URL)\n",
    "#       status, response = http.request(URL)\n",
    "        each_SEP = s.get(URL)\n",
    "        \n",
    "\n",
    "        ## Beaut Soup the response\n",
    "        text = text_from_html(each_SEP.text)\n",
    "\n",
    "        ## Regexp extract relevant details for table\n",
    "        ID = re.findall(r'Small Protein ID (.*?) Sequence',text)\n",
    "        sequence = re.findall(r'Sequence (.*?) Small Protein Length',text)\n",
    "        length = re.findall(r'Small Protein Length (.*?) ORF Type',text)\n",
    "        transcript_ID = re.findall(r'sORF Transcript ID (.*?) Gene Symbol',text)\n",
    "        gene_ID = re.findall(r'Gene Symbol (.*?) Gene Synonyms',text)\n",
    "        location = re.findall(r'Location (.*?) Cell lines',text)\n",
    "        cell_lines = re.findall(r'Cell lines Or Tissues (.*?) Data Source',text)\n",
    "        data_source = re.findall(r'Data Source (.*?) Detailed information',text)\n",
    "        experiment = re.findall(r'Experiment (.*?) Description',text)\n",
    "        experimental_description = re.findall(r'Description (.*?) References',text)\n",
    " \n",
    "\n",
    "        # Create array for dataframe\n",
    "        IDs.append(ID)\n",
    "        Sequence.append(sequence)\n",
    "        Length.append(length)\n",
    "        Transcript_ID.append(transcript_ID)\n",
    "        Gene_ID.append(gene_ID)\n",
    "        Location.append(location)\n",
    "        Cell_Lines.append(cell_lines)\n",
    "        Data_Source.append(data_source)\n",
    "        Experiment.append(experiment)\n",
    "        Experimental_Description.append(experimental_description)\n",
    "       \n",
    "        \n",
    "    current_df = pd.DataFrame([])\n",
    "    current_df['IDs'] = IDs\n",
    "    current_df['Sequence'] = Sequence\n",
    "    current_df['Length'] = Length\n",
    "    current_df['Transcript_ID'] = Transcript_ID\n",
    "    current_df['Gene_ID'] = Gene_ID\n",
    "    current_df['Location'] = Location\n",
    "    current_df['Cell_Lines'] = Cell_Lines\n",
    "    current_df['Data_Source'] = Data_Source\n",
    "    current_df['Experiment'] = Experiment\n",
    "    current_df['Experimental_Description'] = Experimental_Description\n",
    "    \n",
    "    \n",
    "    current_df.to_csv('alldata_page%s' %j)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "start_time = timeit.default_timer()\n",
    "tp = ThreadPoolExecutor(max_workers=total_pages)\n",
    "s = requests.Session()\n",
    "\n",
    "for j in range(0,total_pages):\n",
    "\n",
    "    results_per_page = 10000\n",
    "    orf_type = 'sORF'\n",
    "    gene_type = 'All'\n",
    "    function ='All'\n",
    "    organism = 'human'\n",
    "    datasource = 'All'\n",
    "    page_no = j+1\n",
    "\n",
    "    ## Homepage URL\n",
    "    page_URL = \"http://bioinfo.ibp.ac.cn/SmProt/browse.php?page=%s&perpage=%s&orfType=%s&geneType=%s&function=%s&org=%s&datasource=%s\" %(page_no,results_per_page,orf_type,gene_type,function,organism,datasource)#\n",
    "     \n",
    "    tp.submit(get_page_deets, page_URL)\n",
    "    \n",
    "    \n",
    "tp.shutdown(wait=True)    \n",
    "elapsed = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026.8001635569963"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e5908b518a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurrent_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'current_df' is not defined"
     ]
    }
   ],
   "source": [
    "current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This gets all links on one page ## \n",
    "def get_all_links(page_URL):\n",
    "\n",
    "    response = s.get(page_URL).text\n",
    "    \n",
    "    ## Slower alternatives..\n",
    "    # response = urllib.request.urlopen(URL)\n",
    "    # http = httplib2.Http()\n",
    "    # status, response = http.request(URL)\n",
    "\n",
    "    # all_links = []\n",
    "    # for link in BeautifulSoup(response, parseOnlyThese=SoupStrainer('a')):\n",
    "    #     if link.hasattr('href'):\n",
    "    #         all_links.append(link['href'])\n",
    "\n",
    "\n",
    "    all_links = []\n",
    "    for link in BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer('a')):\n",
    "        try:\n",
    "            all_links.append(link['href'])\n",
    "        except:\n",
    "            pass\n",
    "    ## Filter out other irrelevant links in the page\n",
    "    SM_prot_codes = [u for u in all_links if 'SmProt.php' in u]\n",
    "    #global SM_prot_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test this parallel thread thing - time it as well\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "n_threads = 10 \n",
    "#tp = ThreadPoolExecutor(max_workers=n_threads)\n",
    "s = requests.Session()\n",
    "\n",
    "for i in range(0,10):\n",
    "\n",
    "    results_per_page = 2000\n",
    "    orf_type = 'sORF'\n",
    "    gene_type = 'All'\n",
    "    function ='All'\n",
    "    organism = 'human'\n",
    "    datasource = 'All'\n",
    "    page_no = i\n",
    "\n",
    "    page_URL = \"http://bioinfo.ibp.ac.cn/SmProt/browse.php?page=%s&perpage=%s&orfType=%s&geneType=%s&function=%s&org=%s&datasource=%s\" %(page_no,results_per_page,orf_type,gene_type,function,organism,datasource)#\n",
    "    get_all_links(page_URL)\n",
    "    #tp.submit(get_all_links, page_URL)\n",
    "\n",
    "    \n",
    "#tp.shutdown(wait=True)    \n",
    "elapsed = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.024742046996835"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.294918738989509"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d6774c847c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_next_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    could have new function definition in each iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pinfo2 iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_pages' is not defined"
     ]
    }
   ],
   "source": [
    "# Need to wrap up the scraping of a whole page into one function\n",
    "# Make a function that does the following:\n",
    "\n",
    "# - Each page contains a list of sORFs.\n",
    "# For each page:\n",
    "#               - Go onto that page and get all links to sORF data.\n",
    "#               - Click on each link on that page, parse details and store somewhere\n",
    "\n",
    "\n",
    "# for i in range(0,total_pages):\n",
    "    # could have new function definition in each iteration?? - no need, just have iteration as an input \n",
    "    # into the function. Wait do I even need to do that? Can just define the Page URL outside the function,\n",
    "    # then give the page URL as the input to the function\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define page settings - set all answers as strings \n",
    "# ^^ set these as widgets before pushing to github (http://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html)\n",
    "\n",
    "results_per_page = 10000\n",
    "orf_type = 'sORF'\n",
    "gene_type = 'All'\n",
    "function ='All'\n",
    "organism = 'human'\n",
    "datasource = 'All'\n",
    "\n",
    "## Homepage URL\n",
    "URL = \"http://bioinfo.ibp.ac.cn/SmProt/browse.php?page=1&perpage=%s&orfType=%s&geneType=%s&function=%s&org=%s&datasource=%s\" %(results_per_page,orf_type,gene_type,function,organism,datasource)#\n",
    "\n",
    "## Find number of pages - there are different request libraries to do this - I want the fastest one\n",
    "\n",
    "# response = urllib.request.urlopen(URL)\n",
    "## ***** Start session here ***** ##\n",
    "s = requests.Session()\n",
    "response = s.get(URL)\n",
    "## Beaut Soup the response\n",
    "text = text_from_html(response.text)\n",
    "\n",
    "total_pages_str = re.findall(r'Total Page: (.*?) Total amount:',text)\n",
    "total_pages = literal_eval(total_pages_str[0])\n",
    "\n",
    "## I want to run the scraping function in parallel over each page, so as many parallel functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This gets all links on one page ## \n",
    "def get_all_links(page_URL):\n",
    "\n",
    "    response = s.get(page_URL).text\n",
    "    \n",
    "    ## Slower alternatives..\n",
    "    # response = urllib.request.urlopen(URL)\n",
    "    # http = httplib2.Http()\n",
    "    # status, response = http.request(URL)\n",
    "\n",
    "    # all_links = []\n",
    "    # for link in BeautifulSoup(response, parseOnlyThese=SoupStrainer('a')):\n",
    "    #     if link.hasattr('href'):\n",
    "    #         all_links.append(link['href'])\n",
    "\n",
    "\n",
    "    all_links = []\n",
    "    for link in BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer('a')):\n",
    "        try:\n",
    "            all_links.append(link['href'])\n",
    "        except:\n",
    "            pass\n",
    "    ## Filter out other irrelevant links in the page\n",
    "    SM_prot_codes = [u for u in all_links if 'SmProt.php' in u]\n",
    "    #global SM_prot_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try setting up one thread per page\n",
    "n_threads = total_pages\n",
    "## Parallises the job - divides date range by no. of threads specified probs\n",
    "tp = ThreadPoolExecutor(max_workers=n_threads)\n",
    "# for i in range(0, n_days):\n",
    "#     since_query = self.since + datetime.timedelta(days=i)\n",
    "#     until_query = self.since + datetime.timedelta(days=(i + 1))\n",
    "#     day_query = \"%s since:%s until:%s\" % (query, since_query.strftime(\"%Y-%m-%d\"),\n",
    "#                                           until_query.strftime(\"%Y-%m-%d\"))\n",
    "    tp.submit(self.perform_search, day_query)\n",
    "tp.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future at 0x7efec0346438 state=running>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_threads=82\n",
    "tp = ThreadPoolExecutor(max_workers=n_threads)\n",
    "tp.submit(get_all_links,URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "submit() missing 1 required positional argument: 'fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bc263e71ae38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## Need to wrap the below code into a function - called extract all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: submit() missing 1 required positional argument: 'fn'"
     ]
    }
   ],
   "source": [
    "n_threads = total_pages\n",
    "tp = ThreadPoolExecutor(max_workers=n_threads)\n",
    "for x in range(0,n_threads):\n",
    "    page_no = x\n",
    "    page_URL\n",
    "    tp.submit()\n",
    "\n",
    "## Need to wrap the below code into a function - called extract all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code below should take < 20 mins.. we'll see..\n",
    "## Should also benchmark requests.Session().get request - uses same underlying TCP connection so apparently much faster\n",
    "## Also need to add a timeout to requests to stop everything being slowed by one slow request\n",
    "\n",
    "## Example of parallelising (saved lots of time here: http://elliothallmark.com/2016/12/23/requests-with-concurrent-futures-in-python-2-7/)\n",
    "# import requests\n",
    "# from concurrent.futures import ThreadPoolExecutor, wait, as_completed\n",
    "# from time import time\n",
    "# urls = ['google.com','cnn.com','reddit.com','imgur.com','yahoo.com']\n",
    "# urls = [\"http://\"+url for url in urls]\n",
    "# # Time requests running synchronously\n",
    "# then = time()\n",
    "# sync_results = map(requests.get, urls)\n",
    "# print \"Synchronous done in %s\" % (time()-then)\n",
    "# # Time requests running in threads\n",
    "# then = time()\n",
    "# pool = ThreadPoolExecutor(len(urls))  # for many urls, this should probably be capped at some value.\n",
    "\n",
    "# futures = [pool.submit(requests.get,url) for url in urls]\n",
    "# results = [r.result() for r in as_completed(futures)]\n",
    "# print \"Threadpool done in %s\" % (time()-then)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SM_prot_codes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48e645000b8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://bioinfo.ibp.ac.cn/SmProt/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSM_prot_codes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhttp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttplib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SM_prot_codes' is not defined"
     ]
    }
   ],
   "source": [
    "URL = 'https://bioinfo.ibp.ac.cn/SmProt/' + SM_prot_codes[0]\n",
    "http = httplib2.Http()\n",
    "status, response = http.request(URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "## Need to give this function the URL of each page, and get it to parse each page, find all links\n",
    "## then get all chrom_coords for each page\n",
    "\n",
    "def get_page_deets(page_URL)\n",
    "\n",
    "    ## First get all links on one page\n",
    "    #response = urllib.request.urlopen(URL)\n",
    "#     http = httplib2.Http()\n",
    "#     status, response = http.request(page_URL)\n",
    "\n",
    "    response = s.get(page_URL).text\n",
    "\n",
    "    all_links = []\n",
    "    for link in BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer('a')):\n",
    "        try:\n",
    "            all_links.append(link['href'])\n",
    "        except:\n",
    "            pass\n",
    "    ## Filter out other irrelevant links in the page\n",
    "    SM_prot_codes = [u for u in all_links if 'SmProt.php' in u]\n",
    "\n",
    "\n",
    "\n",
    "    IDs = []\n",
    "    Sequence = []\n",
    "    Length = []\n",
    "    Transcript_ID = []\n",
    "    Gene_ID = []\n",
    "    Location = []\n",
    "    Cell_Lines = []\n",
    "    Data_Source = []\n",
    "    Experiment = []\n",
    "    Experimental_Description = []\n",
    "\n",
    "    http = httplib2.Http()\n",
    "#     start_time = timeit.default_timer()\n",
    "\n",
    "    i = 0\n",
    "    for SM_prot_code in SM_prot_codes:\n",
    "        URL = 'http://bioinfo.ibp.ac.cn/SmProt/' + SM_prot_code\n",
    "        # Think I'll use http.request as faster \n",
    "        #response = urllib.request.urlopen(URL)\n",
    "#       status, response = http.request(URL)\n",
    "        each_SEP = s.get(URL)\n",
    "        \n",
    "\n",
    "        ## Beaut Soup the response\n",
    "        text = text_from_html(each_SEP)\n",
    "\n",
    "        ## Regexp extract relevant details for table\n",
    "        ID = re.findall(r'Small Protein ID (.*?) Sequence',text)\n",
    "        sequence = re.findall(r'Sequence (.*?) Small Protein Length',text)\n",
    "        length = re.findall(r'Small Protein Length (.*?) ORF Type',text)\n",
    "        transcript_ID = re.findall(r'sORF Transcript ID (.*?) Gene Symbol',text)\n",
    "        gene_ID = re.findall(r'Gene Symbol (.*?) Gene Synonyms',text)\n",
    "        location = re.findall(r'Location (.*?) Cell lines',text)\n",
    "        cell_lines = re.findall(r'Cell lines Or Tissues (.*?) Data Source',text)\n",
    "        data_source = re.findall(r'Data Source (.*?) Detailed information',text)\n",
    "        experiment = re.findall(r'Experiment (.*?) Description',text)\n",
    "        experimental_description = re.findall(r'Description (.*?) References',text)\n",
    "        ## Below probably present depending on Data Source (create if loop maybe)\n",
    "    #     PubMed_ref = re.findall(r'PubMed ID (.*?) Title',text)\n",
    "    #     Paper_Title = re.findall(r'Title (.*?) Authors',text)\n",
    "\n",
    "        # Create array for dataframe\n",
    "        IDs.append(ID)\n",
    "        Sequence.append(sequence)\n",
    "        Length.append(length)\n",
    "        Transcript_ID.append(transcript_ID)\n",
    "        Gene_ID.append(gene_ID)\n",
    "        Location.append(location)\n",
    "        Cell_Lines.append(cell_lines)\n",
    "        Data_Source.append(data_source)\n",
    "        Experiment.append(experiment)\n",
    "        Experimental_Description.append(experimental_description)\n",
    "        i += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        \n",
    "    current_df = pd.DataFrame([])\n",
    "    current_df['IDs'] = IDs\n",
    "    current_df['Sequence'] = Sequence\n",
    "    current_df['Length'] = Length\n",
    "    current_df['Transcript_ID'] = Transcript_ID\n",
    "    current_df['Gene_ID'] = Gene_ID\n",
    "    current_df['Location'] = Location\n",
    "    current_df['Cell_Lines'] = Cell_Lines\n",
    "    current_df['Data_Source'] = Data_Source\n",
    "    current_df['Experiment'] = Experiment\n",
    "    current_df['Experimental_Description'] = Experimental_Description\n",
    "    \n",
    "    # Get database number\n",
    "    #db_number = re.findall(r'')\n",
    "    current_df.to_csv('all_data_page%s' %j)\n",
    "    \n",
    "    \n",
    "#     time_elapsed = timeit.default_timer() - start_time\n",
    "    ## Get all links (number just increases by 1 each time), then click on each and webscrape chrom coords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-823211d8a531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mSM_prot_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSM_prot_codes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "s = requests.Session()\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for SM_prot_code in SM_prot_codes[0:10]:\n",
    "    URL = 'http://bioinfo.ibp.ac.cn/SmProt/' + SM_prot_code\n",
    "    response = s.get(URL)\n",
    "    ## Beaut Soup the response\n",
    "    #text = text_from_html(response)\n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Notes\n",
    "\n",
    "# below code shows http.request is ~2x faster than urllib requests or requests\n",
    "# compare http, requests, and urllib with timeit\n",
    "import timeit\n",
    "# timeit.timeit(urllib.request.urlopen(URL))\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for SM_prot_code in SM_prot_codes[0:10]:\n",
    "    URL = 'http://bioinfo.ibp.ac.cn/SmProt/' + SM_prot_code\n",
    "    response = urllib.request.urlopen(URL)\n",
    "    ## Beaut Soup the response\n",
    "    text = text_from_html(response)\n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)\n",
    "\n",
    "# compare http, requests, and urllib with timeit\n",
    "import timeit\n",
    "import requests\n",
    "# timeit.timeit(urllib.request.urlopen(URL))\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for SM_prot_code in SM_prot_codes[0:10]:\n",
    "    URL = 'http://bioinfo.ibp.ac.cn/SmProt/' + SM_prot_code\n",
    "    response = requests.get(URL)\n",
    "    ## Beaut Soup the response\n",
    "    #text = text_from_html(response)\n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)\n",
    "\n",
    "\n",
    "# compare http, requests, and urllib with timeit\n",
    "import timeit\n",
    "import requests\n",
    "import httplib2\n",
    "# timeit.timeit(urllib.request.urlopen(URL))\n",
    "http = httplib2.Http()\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for SM_prot_code in SM_prot_codes[0:10]:\n",
    "    URL = 'http://bioinfo.ibp.ac.cn/SmProt/' + SM_prot_code\n",
    "    \n",
    "    status, response = http.request(URL)\n",
    "    ## Beaut Soup the response\n",
    "    text = text_from_html(response)\n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.21577886200248\n"
     ]
    }
   ],
   "source": [
    "## Quick comparison between requests sessions and httplib2.http.request\n",
    "## UPDATE - looks like requests.Session wins!\n",
    "import timeit\n",
    "\n",
    "results_per_page = 2000\n",
    "orf_type = 'sORF'\n",
    "gene_type = 'All'\n",
    "function ='All'\n",
    "organism = 'human'\n",
    "datasource = 'All'\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(0,15):\n",
    "    page_no = i \n",
    "    URL = \"http://bioinfo.ibp.ac.cn/SmProt/browse.php?page=%s&perpage=%s&orfType=%s&geneType=%s&function=%s&org=%s&datasource=%s\" %(page_no,results_per_page,orf_type,gene_type,function,organism,datasource)#\n",
    "    response = s.get(URL)\n",
    "    text = response.text\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Quick comparison between requests sessions and httplib2.http.request\n",
    "import timeit\n",
    "\n",
    "results_per_page = 2000\n",
    "orf_type = 'sORF'\n",
    "gene_type = 'All'\n",
    "function ='All'\n",
    "organism = 'human'\n",
    "datasource = 'All'\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(0,15):\n",
    "    page_no = i \n",
    "    URL = \"http://bioinfo.ibp.ac.cn/SmProt/browse.php?page=%s&perpage=%s&orfType=%s&geneType=%s&function=%s&org=%s&datasource=%s\" %(page_no,results_per_page,orf_type,gene_type,function,organism,datasource)#\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
